{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ff659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, iqr\n",
    "from config.config_loader import load_config\n",
    "\n",
    "# Load configuration file for feature engineering\n",
    "CONFIG_DATA = load_config(os.path.join(\"config\", \"feature_engineering_config.json\"))\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    \"\"\"\n",
    "    Read driving behavior data from TXT file.\n",
    "    Fields:\n",
    "        timestamp, steering, throttle, brake\n",
    "    Additional features:\n",
    "        First and second derivatives (rate of change)\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        contents = f.readlines()\n",
    "\n",
    "    # Extract values from structured format\n",
    "    datas = [re.findall(r'.*= (.*)  .*= (.*)  .*= (.*)  .*= (.*)', line)[0]\n",
    "             for line in contents]\n",
    "    \n",
    "    datas = pd.DataFrame(datas, columns=['timestamp', 'steering', 'throttle', 'brake'],\n",
    "                         dtype=float)\n",
    "\n",
    "    # First and second derivatives of steering\n",
    "    datas['steering_derivative'] = np.gradient(datas['steering'], datas['timestamp'])\n",
    "    datas['steering_second_derivative'] = np.gradient(datas['steering_derivative'],\n",
    "                                                      datas['timestamp'])\n",
    "\n",
    "    # First and second derivatives of throttle\n",
    "    datas['throttle_derivative'] = np.gradient(datas['throttle'], datas['timestamp'])\n",
    "    datas['throttle_second_derivative'] = np.gradient(datas['throttle_derivative'],\n",
    "                                                      datas['timestamp'])\n",
    "\n",
    "    # First and second derivatives of brake\n",
    "    datas['brake_derivative'] = np.gradient(datas['brake'], datas['timestamp'])\n",
    "    datas['brake_second_derivative'] = np.gradient(datas['brake_derivative'],\n",
    "                                                   datas['timestamp'])\n",
    "\n",
    "    return datas\n",
    "\n",
    "\n",
    "def read_speed_data(path, start_timestamp):\n",
    "    \"\"\"\n",
    "    Read speed data from TXT file.\n",
    "    Handles missing values such as 'null'.\n",
    "    Computes first and second derivatives of speed.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        contents = f.readlines()\n",
    "\n",
    "    speed_data = []\n",
    "    for line in contents:\n",
    "        match = re.findall(r'(.*) (.*)', line)\n",
    "        if match:\n",
    "            timestamp, speed = match[0]\n",
    "\n",
    "            # Convert missing speed to nan\n",
    "            speed = np.nan if speed == 'null' else int(speed)\n",
    "\n",
    "            # Synchronize timestamp\n",
    "            timestamp = int(timestamp) / 25 + start_timestamp\n",
    "\n",
    "            speed_data.append((timestamp, speed))\n",
    "\n",
    "    speed_data = pd.DataFrame(speed_data, columns=['timestamp', 'speed'])\n",
    "\n",
    "    # First and second derivatives of speed\n",
    "    speed_data['speed_derivative'] = np.gradient(speed_data['speed'],\n",
    "                                                 speed_data['timestamp'])\n",
    "    speed_data['speed_second_derivative'] = np.gradient(speed_data['speed_derivative'],\n",
    "                                                        speed_data['timestamp'])\n",
    "\n",
    "    return speed_data\n",
    "\n",
    "\n",
    "def csv_reader(name, start_time):\n",
    "    \"\"\"\n",
    "    Read eye-tracking data from CSV file.\n",
    "    Selected features:\n",
    "        - gaze point coordinates\n",
    "        - head position (derived from eye positions)\n",
    "        - pupil size\n",
    "        - eye movement type\n",
    "    \n",
    "    Additional computed features:\n",
    "        - gaze velocity and acceleration\n",
    "        - head motion velocity and acceleration\n",
    "    \"\"\"\n",
    "    columns_to_read = [\n",
    "        'Recording timestamp',\n",
    "        'Event', 'Recording date', 'Recording start time',\n",
    "        'Validity left', 'Validity right',\n",
    "        'Gaze point X', 'Gaze point Y',\n",
    "        'Gaze point left X', 'Gaze point left Y',\n",
    "        'Gaze point right X', 'Gaze point right Y',\n",
    "        'Eye position left X (DACSmm)', 'Eye position left Y (DACSmm)', 'Eye position left Z (DACSmm)',\n",
    "        'Eye position right X (DACSmm)', 'Eye position right Y (DACSmm)', 'Eye position right Z (DACSmm)',\n",
    "        'Eye movement type', 'Gaze event duration',\n",
    "        'Pupil diameter left', 'Pupil diameter right'\n",
    "    ]\n",
    "\n",
    "    df = pd.read_csv(name, usecols=columns_to_read, low_memory=False)\n",
    "\n",
    "    # Select required columns and rename\n",
    "    df_new = df[['Recording timestamp', 'Gaze point X', 'Gaze point Y',\n",
    "                 'Eye position left X (DACSmm)', 'Eye position left Y (DACSmm)',\n",
    "                 'Eye position left Z (DACSmm)', 'Eye position right X (DACSmm)',\n",
    "                 'Eye position right Y (DACSmm)', 'Eye position right Z (DACSmm)']].copy()\n",
    "\n",
    "    df_new.columns = ['timestamp', 'Gaze point X', 'Gaze point Y',\n",
    "                      'Head left X', 'Head left Y', 'Head left Z',\n",
    "                      'Head right X', 'Head right Y', 'Head right Z']\n",
    "\n",
    "    # Convert timestamp to seconds and synchronize\n",
    "    df_new['timestamp'] = df_new['timestamp'] / 1e6 + start_time\n",
    "\n",
    "    # Gaze velocity and acceleration (X, Y)\n",
    "    for axis in ['X', 'Y']:\n",
    "        df_new[f'Gaze_{axis}_vel'] = np.gradient(df_new[f'Gaze point {axis}'],\n",
    "                                                 df_new['timestamp'])\n",
    "        df_new[f'Gaze_{axis}_acc'] = np.gradient(df_new[f'Gaze_{axis}_vel'],\n",
    "                                                 df_new['timestamp'])\n",
    "\n",
    "    # Eye movement speed and acceleration (euclidean)\n",
    "    df_new['eye_speed'] = np.sqrt(df_new['Gaze point X'].diff()**2 +\n",
    "                                  df_new['Gaze point Y'].diff()**2) / df_new['timestamp'].diff()\n",
    "\n",
    "    df_new['eye_acceleration'] = np.gradient(df_new['eye_speed'], df_new['timestamp'])\n",
    "\n",
    "    # Compute head center position\n",
    "    df['Head X'] = df[['Eye position left X (DACSmm)', 'Eye position right X (DACSmm)']].mean(axis=1)\n",
    "    df['Head Y'] = df[['Eye position left Y (DACSmm)', 'Eye position right Y (DACSmm)']].mean(axis=1)\n",
    "    df['Head Z'] = df[['Eye position left Z (DACSmm)', 'Eye position right Z (DACSmm)']].mean(axis=1)\n",
    "\n",
    "    # Head velocity and acceleration (X, Y, Z)\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        df_new[f'Head_vel_{axis}'] = np.gradient(df[f'Head {axis}'].values,\n",
    "                                                 df_new['timestamp'])\n",
    "        df_new[f'Head_acc_{axis}'] = np.gradient(df_new[f'Head_vel_{axis}'],\n",
    "                                                 df_new['timestamp'])\n",
    "\n",
    "    # Combined velocity and acceleration\n",
    "    df_new['Head_vel_total'] = np.sqrt(df_new['Head_vel_X']**2 +\n",
    "                                       df_new['Head_vel_Y']**2 +\n",
    "                                       df_new['Head_vel_Z']**2)\n",
    "\n",
    "    df_new['Head_acc_total'] = np.sqrt(df_new['Head_acc_X']**2 +\n",
    "                                       df_new['Head_acc_Y']**2 +\n",
    "                                       df_new['Head_acc_Z']**2)\n",
    "\n",
    "    # Additional raw signals\n",
    "    df_new['Pupil_diameter_left'] = df['Pupil diameter left']\n",
    "    df_new['Pupil_diameter_right'] = df['Pupil diameter right']\n",
    "    df_new['Eye movement type'] = df['Eye movement type']\n",
    "\n",
    "    # Debug outlier detection for eye movement speed\n",
    "    large_values = df_new[df_new['eye_speed'].abs() > 100000]\n",
    "    if not large_values.empty:\n",
    "        print(\"Detected extremely large eye movement speeds:\")\n",
    "        print(large_values[['eye_speed', 'timestamp']])\n",
    "\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202d1c2-4266-4dfc-829c-3a12797e3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hampel_filter_forloop_numba(input_series, window_size, n_sigmas=3):\n",
    "    \"\"\"\n",
    "    Hampel filter for outlier detection and replacement.\n",
    "    Robust to non-Gaussian noise because it uses median and MAD.\n",
    "    \n",
    "    Args:\n",
    "        input_series (np.ndarray): 1D signal array.\n",
    "        window_size (int): Half window size for sliding median filter.\n",
    "        n_sigmas (int or float): Threshold multiplier for MAD-based scale.\n",
    "        \n",
    "    Returns:\n",
    "        new_series (np.ndarray): Filtered signal with outliers replaced.\n",
    "        indices (list): Indices where outliers were detected.\n",
    "    \"\"\"\n",
    "    n = len(input_series)\n",
    "    new_series = input_series.copy()\n",
    "    k = 1.4826  # scale factor converting MAD to std for Gaussian assumption\n",
    "    indices = []\n",
    "\n",
    "    # Loop through all valid center indices\n",
    "    for i in range(window_size, (n - window_size)):\n",
    "        # Compute the local median within the window\n",
    "        x0 = np.nanmedian(input_series[(i - window_size):(i + window_size)])\n",
    "        \n",
    "        # Compute the Median Absolute Deviation (MAD) and scale to std estimate\n",
    "        window_data = input_series[(i - window_size):(i + window_size)]\n",
    "        S0 = k * np.nanmedian(np.abs(window_data - x0))\n",
    "        \n",
    "        # Check if the current point deviates more than n_sigmas * S0\n",
    "        if np.abs(input_series[i] - x0) > n_sigmas * S0:\n",
    "            # Replace detected outliers with the local median\n",
    "            new_series[i] = x0\n",
    "            indices.append(i)\n",
    "\n",
    "    return new_series, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5a750-14b8-4928-813b-0c7f15d3b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RAW_DATA_DIR = Path(\"raw_data\")\n",
    "\n",
    "# Columns to interpolate (numeric sensor signals)\n",
    "VALUE_COLS_TO_INTERPOLATE = [\n",
    "    \"Gaze point left X\", \"Gaze point left Y\",\n",
    "    \"Gaze point right X\", \"Gaze point right Y\",\n",
    "    \"Eye position left X (DACSmm)\", \"Eye position left Y (DACSmm)\", \"Eye position left Z (DACSmm)\",\n",
    "    \"Eye position right X (DACSmm)\", \"Eye position right Y (DACSmm)\", \"Eye position right Z (DACSmm)\",\n",
    "    \"Pupil diameter left\", \"Pupil diameter right\"\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_and_interpolate_file(path: Path, save_processed: bool = False):\n",
    "    \"\"\"\n",
    "    Data preprocessing pipeline for eye-tracking CSV files.\n",
    "\n",
    "    Processing workflow:\n",
    "    1. Remove rows before the event `ScreenRecordingStart`\n",
    "    2. Rows with any non-empty event → set numeric value columns to NaN\n",
    "    3. Rows where both eyes are marked as `Invalid` → set numeric value columns to NaN\n",
    "    4. Convert value columns to numeric, forcing invalid strings to NaN\n",
    "    5. Interpolate short missing gaps (≤ 5 samples) only\n",
    "    6. Forward/backward fill remaining boundaries\n",
    "    7. Apply Hampel filter to remove outliers\n",
    "    8. Recompute the binocular gaze point `Gaze point X/Y` as average of left/right\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Processed DataFrame\n",
    "        loss_rate (float): Invalid data ratio before interpolation\n",
    "        total_outlier_count (int): Number of values replaced by Hampel filter\n",
    "        outlier_indices_by_column (dict): Outlier indices for each column\n",
    "    \"\"\"\n",
    "    outlier_indices_by_column = {}\n",
    "\n",
    "    df = pd.read_csv(path, dtype=str, low_memory=False)\n",
    "\n",
    "    # Keep data only after `ScreenRecordingStart`\n",
    "    calibration_start_idx = df[df[\"Event\"] == \"ScreenRecordingStart\"].index[0]\n",
    "    df = df.iloc[calibration_start_idx + 1:].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Keep only rows produced by the eye tracker sensor\n",
    "    df = df[df[\"Sensor\"] == \"Eye Tracker\"].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    total_count = len(df)\n",
    "\n",
    "    # Non-empty event rows → considered unreliable → set as NaN\n",
    "    event_nonempty_mask = df[\"Event\"].notna() & (df[\"Event\"].str.strip() != \"\")\n",
    "    df.loc[event_nonempty_mask, VALUE_COLS_TO_INTERPOLATE] = np.nan\n",
    "\n",
    "    # Both eyes invalid → treat measurements as missing\n",
    "    invalid_mask = (df[\"Validity left\"] == \"Invalid\") & (df[\"Validity right\"] == \"Invalid\")\n",
    "    df.loc[invalid_mask, VALUE_COLS_TO_INTERPOLATE] = np.nan\n",
    "\n",
    "    invalid_count = event_nonempty_mask.sum() + invalid_mask.sum()\n",
    "    loss_rate = invalid_count / total_count\n",
    "\n",
    "    # Convert all value columns to numeric, coercing any errors into NaN\n",
    "    for col in VALUE_COLS_TO_INTERPOLATE:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Limit interpolation to short NaN gaps only\n",
    "    max_gap = 5\n",
    "\n",
    "    for col in VALUE_COLS_TO_INTERPOLATE:\n",
    "        s = df[col]\n",
    "        is_na = s.isna()\n",
    "\n",
    "        # Identify consecutive NaN segments\n",
    "        seg_id = (is_na != is_na.shift()).cumsum()\n",
    "        groups = s.groupby(seg_id)\n",
    "\n",
    "        new_s = s.copy()\n",
    "\n",
    "        for _, g in groups:\n",
    "            if g.isna().all():\n",
    "                gap_len = len(g)\n",
    "                # Interpolate only if gap is short enough\n",
    "                if gap_len <= max_gap:\n",
    "                    new_s[g.index] = s.interpolate(method=\"linear\", limit_direction=\"both\")[g.index]\n",
    "                else:\n",
    "                    new_s[g.index] = np.nan\n",
    "\n",
    "        df[col] = new_s\n",
    "\n",
    "    # Boundary fill to handle beginning and end NaNs\n",
    "    df[VALUE_COLS_TO_INTERPOLATE] = (\n",
    "        df[VALUE_COLS_TO_INTERPOLATE].ffill().bfill()\n",
    "    )\n",
    "\n",
    "    # Hampel filter for robust outlier detection\n",
    "    total_outlier_count = 0\n",
    "    for col in VALUE_COLS_TO_INTERPOLATE:\n",
    "        series = df[col].to_numpy()\n",
    "        filtered_series, indices = hampel_filter_forloop_numba(\n",
    "            series, window_size=10, n_sigmas=3\n",
    "        )\n",
    "        df[col] = filtered_series\n",
    "        total_outlier_count += len(indices)\n",
    "        outlier_indices_by_column[col] = indices\n",
    "\n",
    "    # Recompute gaze coordinates as binocular average\n",
    "    df[\"Gaze point X\"] = df[[\"Gaze point left X\", \"Gaze point right X\"]].mean(axis=1)\n",
    "    df[\"Gaze point Y\"] = df[[\"Gaze point left Y\", \"Gaze point right Y\"]].mean(axis=1)\n",
    "\n",
    "    return df, loss_rate, total_outlier_count, outlier_indices_by_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efc85d-35f1-4e59-af21-21fcae01d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Load configuration file ===\n",
    "CONFIG_PATH = Path(\"config\") / \"feature_engineering_config.json\"\n",
    "with CONFIG_PATH.open(encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "def batch_process_csv_interpolation():\n",
    "    \"\"\"\n",
    "    Batch processing pipeline:\n",
    "    - Iterate through configured dataset groups and experiments\n",
    "    - Apply preprocessing and interpolation function for each file\n",
    "    - Save cleaned outputs into target directories\n",
    "    \"\"\"\n",
    "\n",
    "    for group_key in [\"GroupA_data_path\", \"GroupB_data_path\"]:\n",
    "        group_data = config.get(group_key, {})\n",
    "        print(f\"\\n[INFO] Processing dataset group: {group_key}\")\n",
    "\n",
    "        for ex in [\"ex1\", \"ex2\"]:\n",
    "            in_key = f\"csv_path_{ex}\"\n",
    "            out_key = f\"csv_path_{ex}_cleaned\"\n",
    "\n",
    "            input_list = group_data.get(in_key, [])\n",
    "            output_list = group_data.get(out_key, [])\n",
    "\n",
    "            print(f\"[INFO] Experiment {ex}: {len(input_list)} files to process\")\n",
    "\n",
    "            for in_path_str, out_path_str in zip(input_list, output_list):\n",
    "                if not in_path_str:\n",
    "                    continue\n",
    "\n",
    "                in_path = Path(in_path_str)\n",
    "                out_path = Path(out_path_str)\n",
    "\n",
    "                # Apply preprocessing\n",
    "                df, loss_rate, n_outliers, outlier_indices_by_column = \\\n",
    "                    preprocess_and_interpolate_file(in_path)\n",
    "\n",
    "                print(\"[INFO] Hampel filter applied\")\n",
    "\n",
    "                # Ensure output directory exists\n",
    "                out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Save processed output\n",
    "                df.to_csv(out_path, index=False)\n",
    "                print(\n",
    "                    f\"[INFO] Saved cleaned file: {out_path} \"\n",
    "                    f\"(invalid rate: {loss_rate:.2%}, outliers: {n_outliers})\"\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_process_csv_interpolation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Root directory to save merged results\n",
    "SAVE_ROOT = \"data_pro1\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# Dataset groups and participant counts (keep original identifiers: NC / PD)\n",
    "groups = {\n",
    "    \"NC\": CONFIG_DATA[\"NC_number\"],\n",
    "    \"PD\": CONFIG_DATA[\"PD_number\"]\n",
    "}\n",
    "\n",
    "# Experiment identifiers\n",
    "ex_num_list = [\"ex1\", \"ex2\"]\n",
    "\n",
    "# Sensor data modalities\n",
    "sensor_types = [\"pedal\", \"speed\", \"eyemovement\"]\n",
    "\n",
    "\n",
    "for group, person_count in groups.items():\n",
    "    data_path_key = f\"{group}_data_path\"\n",
    "\n",
    "    for ex_num in ex_num_list:\n",
    "        for sensor in sensor_types:\n",
    "\n",
    "            print(f\"[INFO] Processing group={group}, experiment={ex_num}, sensor={sensor}\")\n",
    "\n",
    "            merged_dfs = []\n",
    "\n",
    "            for person_idx in range(person_count):\n",
    "\n",
    "                # Special-case skip condition in original dataset\n",
    "                if group == \"PD\" and ex_num == \"ex1\" and person_idx == 8:\n",
    "                    continue\n",
    "\n",
    "                # Retrieve required paths and timestamps from config\n",
    "                pedal_data_path = CONFIG_DATA[data_path_key][f\"pedal_path_{ex_num}\"][person_idx]\n",
    "                speed_data_path = CONFIG_DATA[data_path_key][f\"speed_path_{ex_num}\"][person_idx]\n",
    "                start_timestamp = CONFIG_DATA[data_path_key][f\"video_start_timestamp_{ex_num}\"][person_idx]\n",
    "                em_csv_path = CONFIG_DATA[data_path_key][f\"csv_path_{ex_num}_cleaned\"][person_idx]\n",
    "\n",
    "                # Load sensor data based on type\n",
    "                if sensor == \"pedal\":\n",
    "                    df = read_txt(pedal_data_path)\n",
    "                elif sensor == \"speed\":\n",
    "                    df = read_speed_data(speed_data_path, start_timestamp)\n",
    "                elif sensor == \"eyemovement\":\n",
    "                    df = csv_reader(em_csv_path, start_timestamp)\n",
    "\n",
    "                # Assign participant ID (sequential index)\n",
    "                df[\"person_id\"] = person_idx + 1\n",
    "                merged_dfs.append(df)\n",
    "\n",
    "            # Merge all participants in the group\n",
    "            result_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "\n",
    "            # File naming convention preserved\n",
    "            save_filename = f\"{group}_{sensor}_data_{ex_num}.csv\"\n",
    "            save_path = os.path.join(SAVE_ROOT, save_filename)\n",
    "\n",
    "            # Persist output to disk\n",
    "            result_df.to_csv(save_path, index=False)\n",
    "\n",
    "            print(f\"[INFO] Saved: {save_path}  (records={len(result_df)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
