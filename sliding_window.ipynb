{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529b7cca",
   "metadata": {},
   "source": [
    "## 特征计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#window_size = int(60*21.25)\n",
    "# WINDOW_SIZE = 1275\n",
    "# WINDOW_STEP = 21\n",
    "# 0.5s 11\n",
    "# 1s 21\n",
    "# 2s 43\n",
    "# 5s 108\n",
    "# 150s \n",
    "WINDOW_SIZE = 1275\n",
    "WINDOW_STEP = 21\n",
    "output_name= \"feature_120_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "765fdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, iqr\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "import pywt\n",
    "\n",
    "#频域特征计算\n",
    "def get_fft_stats(df, columns=['steering', 'throttle', 'brake']):\n",
    "    features = {}\n",
    "\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        signal = df[col].dropna().values  # 去掉 NaN\n",
    "\n",
    "        if len(signal) < 2:\n",
    "            continue  # 跳过太短的信号\n",
    "\n",
    "        # 1. 做 FFT（只取实数部分频谱）\n",
    "        fft_vals = np.abs(fft(signal))\n",
    "        fft_vals = fft_vals[:len(fft_vals)//2]  # 保留正频率部分\n",
    "\n",
    "        # 2. 提取特征\n",
    "        features[f'FFT_{col}_mean'] = np.mean(fft_vals)\n",
    "        features[f'FFT_{col}_std'] = np.std(fft_vals)\n",
    "        features[f'FFT_{col}_max'] = np.max(fft_vals)\n",
    "        features[f'FFT_{col}_min'] = np.min(fft_vals)\n",
    "        features[f'FFT_{col}_energy'] = np.sum(fft_vals ** 2)\n",
    "\n",
    "        # 3. 频谱熵（归一化再计算熵）\n",
    "        psd = fft_vals ** 2\n",
    "        psd_norm = psd / np.sum(psd) if np.sum(psd) != 0 else psd\n",
    "        features[f'FFT_{col}_entropy'] = entropy(psd_norm)\n",
    "\n",
    "    return features\n",
    "\n",
    "#小波变换参数特征计算\n",
    "def extract_wavelet_features(df, columns=['steering', 'throttle', 'brake'], wavelet='db4', level=3):\n",
    "    features = {}\n",
    "\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        signal = df[col].dropna().values\n",
    "\n",
    "        if len(signal) < 2:\n",
    "            continue\n",
    "\n",
    "        # 小波分解（返回 approximation 和 detail 系数）\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "\n",
    "        for i, coeff in enumerate(coeffs):\n",
    "            prefix = f'{col}_L{i}'\n",
    "\n",
    "            # 统计特征\n",
    "            features[f'WVL_{prefix}_mean'] = np.mean(coeff)\n",
    "            features[f'WVL_{prefix}_std'] = np.std(coeff)\n",
    "            features[f'WVL_{prefix}_max'] = np.max(coeff)\n",
    "            features[f'WVL_{prefix}_energy'] = np.sum(np.square(coeff))\n",
    "\n",
    "            # 熵（表示信号复杂度）\n",
    "            coeff_norm = np.square(coeff) / np.sum(np.square(coeff)) if np.sum(np.square(coeff)) > 0 else coeff\n",
    "            features[f'WVL_{prefix}_entropy'] = entropy(coeff_norm)\n",
    "\n",
    "    return features\n",
    "\n",
    "#上一篇文章统计特征计算\n",
    "def extract_pedal(pedal_df:pd.DataFrame):\n",
    "    res = {\n",
    "        \"LAST_steer_sum\" : np.nan,\n",
    "        \"LAST_steer_abs_sum\" : np.nan,\n",
    "        \"LAST_steer_flucuate_times\" : np.nan,\n",
    "        \"LAST_steer_sum_per_fulct\" : np.nan,\n",
    "        \"LAST_steer_max_fluct\" : np.nan,\n",
    "        \"LAST_steer_mean_fluct_speed\" : np.nan,\n",
    "        \"LAST_steer_max_fluct_speed\" : np.nan,\n",
    "        \"LAST_throttle_duration\" : np.nan,\n",
    "        \"LAST_brake_duration\" : np.nan,\n",
    "        \"LAST_throttle_brake_ratio\" : np.nan,\n",
    "        \"LAST_brake_times\" : np.nan,\n",
    "        \"LAST_throttle_auc\" : np.nan,\n",
    "    }\n",
    "    if pedal_df.shape[0]==0:\n",
    "        return res\n",
    "    \n",
    "    pedal_df = pedal_df.copy()\n",
    "    pedal_df[\"throttle\"] = -pedal_df[\"throttle\"] + 32767.0\n",
    "    pedal_df = pedal_df.copy()\n",
    "    pedal_df[\"brake\"] = -pedal_df[\"brake\"] + 32767.0\n",
    "\n",
    "    res[\"LAST_steer_sum\"] = pedal_df[\"steering\"].sum()\n",
    "    res[\"LAST_steer_abs_sum\"] = np.abs(pedal_df[\"steering\"]).sum()\n",
    "    try:\n",
    "        # res[\"flucuate_times\"] = pedal_df[\"steering_derivative\"].value_counts()[0]\n",
    "        pedal_df[\"LAST_pass_zero\"] = (pedal_df[\"steering\"] * pedal_df[\"steering\"].shift(1)).map(lambda x: 1 if x<=0 else 0)\n",
    "        res[\"LAST_steer_flucuate_times\"] = pedal_df[\"LAST_pass_zero\"].sum()\n",
    "        if res[\"LAST_steer_flucuate_times\"] == 0:\n",
    "            res[\"LAST_steer_sum_per_fulct\"] = 0\n",
    "        else:\n",
    "            res[\"LAST_steer_sum_per_fulct\"] = res[\"LAST_steer_abs_sum\"] / res[\"LAST_flucuate_times\"]\n",
    "    except:\n",
    "        res[\"LAST_flucuate_times\"] = 0\n",
    "        res[\"LAST_steer_sum_per_fulct\"] = 0\n",
    "    res[\"LAST_steer_max_fluct\"] = np.abs(pedal_df[\"steering\"]).max()\n",
    "    res[\"LAST_steer_mean_fluct_speed\"] = np.nanmean(np.abs(pedal_df[\"steering_derivative\"]))\n",
    "    res[\"LAST_steer_max_fluct_speed\"] = np.abs(pedal_df[\"steering_derivative\"]).max()\n",
    "    \n",
    "    pedal_df[\"dura\"] = pedal_df[\"timestamp\"].shift(-1) - pedal_df[\"timestamp\"]\n",
    "    res[\"LAST_throttle_duration\"] = pedal_df[~(pedal_df[\"throttle\"]==0)][\"dura\"].sum()\n",
    "    res[\"LAST_brake_duration\"] = pedal_df[~(pedal_df[\"brake\"]==0)][\"dura\"].sum()\n",
    "    res[\"LAST_throttle_brake_ratio\"] = res[\"LAST_throttle_duration\"] / (res[\"LAST_brake_duration\"]+0.01)\n",
    "\n",
    "    _temp_times=0\n",
    "    _last_idx=pedal_df.index[0]\n",
    "    for idx, row in pedal_df[pedal_df[\"brake\"]==0].iterrows():\n",
    "        if (not idx-_last_idx==1) and (not idx-_last_idx==0):\n",
    "            _temp_times+=1\n",
    "        _last_idx=idx\n",
    "    res[\"LAST_brake_times\"] = _temp_times\n",
    "    res[\"LAST_throttle_auc\"] = pedal_df[\"throttle\"].sum()\n",
    "    return res\n",
    "\n",
    "#新特征\n",
    "def extract_new_pedal_feature(pedal_df:pd.DataFrame):\n",
    "    res = {\n",
    "\n",
    "        \"NewPedal_steering_Change_quantile_mean\" : np.nan,\n",
    "        \"NewPedal_steering_fft_Kurtosis\": np.nan,\n",
    "        \"NewPedal_steering_unique_value_ratio\": np.nan,\n",
    "        \"NewPedal_steering_FFT_Aggregated_Centroid\":np.nan\n",
    "    }\n",
    "    if pedal_df.shape[0]==0:\n",
    "        return res\n",
    "    \n",
    "    #————————计算参数一 Steering – Change quantile mean：——————————————\n",
    "    #Steering – Change quantile mean：The average absolute value of the changes in the time series.\n",
    "    # 也就是说，它是计算方向盘角度（steering）随时间变化的变化值的绝对值的平均数，可以理解为方向盘活动的“平均强度”。\n",
    "\n",
    "    # 计算相邻方向盘值的变化（差值）\n",
    "    steering_diff = pedal_df[\"steering\"].diff()\n",
    "\n",
    "    # 计算变化的绝对值\n",
    "    steering_abs_change = steering_diff.abs()\n",
    "\n",
    "    # 时间差（单位：秒）\n",
    "    time_diff = pedal_df[\"timestamp\"].diff()\n",
    "\n",
    "    # 防止除以0\n",
    "    time_diff.replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # 方向盘变化率（角速度） - 绝对值\n",
    "    steering_rate = steering_abs_change / time_diff\n",
    "\n",
    "    # 求平均\n",
    "    steering_rate_mean = steering_rate.mean()\n",
    "\n",
    "    res[\"NewPedal_steering_Change_quantile_mean\"] = steering_rate_mean\n",
    "\n",
    "    #————————计算参数二 Steering – FFT aggregated kurtosis：——————————————\n",
    "    #Steering – FFT aggregated kurtosis\n",
    "    # 即：“方向盘数据的时序信号经过傅里叶变换（FFT）后，计算其频谱的峭度（Kurtosis）”。\n",
    "    # 这个特征可以帮助你分析方向盘信号中频率分布的尖峭程度，比如是否包含突变或异常的高频行为。\n",
    "\n",
    "    # 1. 取 steering 数据（去掉 NaN）\n",
    "    steering_signal = pedal_df[\"steering\"].dropna().values\n",
    "\n",
    "    # 2. 快速傅里叶变换（FFT）\n",
    "    steering_fft = fft(steering_signal)\n",
    "\n",
    "    # 3. 取 FFT 的幅度谱（即复数的模长）\n",
    "    fft_magnitude = np.abs(steering_fft)\n",
    "\n",
    "    # 4. 计算频谱的峭度（默认 Fisher=False 为“实际峭度”）\n",
    "    fft_kurtosis = kurtosis(fft_magnitude, fisher=False)\n",
    "\n",
    "    res[\"NewPedal_steering_fft_Kurtosis\"] = fft_kurtosis\n",
    "\n",
    "    #————————计算参数三 Steering – unique value ratio：——————————————\n",
    "    # \"Steering Ratio\"（方向盘数据唯一值与时间序列总长度的比率）\n",
    "    # 这个比率衡量的是方向盘数据的\"独特性\" - 值越接近1表示数据变化越大，越接近0表示数据重复性越高\n",
    "\n",
    "    # 获取方向盘数据列\n",
    "    steering_data = pedal_df[\"steering\"]\n",
    "    \n",
    "    # 计算唯一值数量\n",
    "    unique_values = steering_data.nunique()\n",
    "    \n",
    "    # 计算时间序列总长度\n",
    "    total_length = len(steering_data)\n",
    "    \n",
    "    # 计算比率\n",
    "    ratio = unique_values / total_length\n",
    "\n",
    "    res[\"NewPedal_steering_unique_value_ratio\"] = ratio\n",
    "\n",
    "    #————————计算参数四 Steering FFT Aggregated Centroid（方向盘数据的FFT聚合质心频率）：——————————————\n",
    "    # 该参数表示 方向盘信号（Steering）频谱的能量集中趋势，计算方式为对时间序列进行快速傅里叶变换（FFT），然后求其频谱的加权平均频率（质心频率）。\n",
    "    # 物理意义：反映方向盘变化的主要频率成分（例如高频抖动 vs 低频平滑转向）。\n",
    "    # 应用场景：检测方向盘控制的稳定性（高频成分多可能表示抖动或频繁修正）。\n",
    "\n",
    "    # 计算FFT\n",
    "    n = len(steering_signal)\n",
    "    fft_values = fft(steering_signal)\n",
    "    fft_magnitude = np.abs(fft_values)[:n//2]  # 取单边频谱\n",
    "    \n",
    "    # 生成频率轴\n",
    "    freqs = np.fft.fftfreq(n, d=1.0/21.75)[:n//2]\n",
    "    \n",
    "    # 计算质心频率（加权平均频率）\n",
    "    if np.sum(fft_magnitude) > 0:\n",
    "        centroid = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    else:\n",
    "        centroid = 0.0  # 避免除以零\n",
    "\n",
    "    res[\"NewPedal_steering_FFT_Aggregated_Centroid\"] = centroid\n",
    "\n",
    "    return res\n",
    "\n",
    "#新特征\n",
    "def extract_new_speed_feature(speed_df:pd.DataFrame):\n",
    "    res = {\n",
    "        \"NewSpeed_90%_quantile\" :np.nan,\n",
    "        \"NewSpeed_C3\":np.nan,\n",
    "        \"NewSpeed_change_quantile_variance\":np.nan\n",
    "    }\n",
    "    if speed_df.shape[0]==0:\n",
    "        return res\n",
    "    \n",
    "    #————————计算参数 Speed 90% quantile（计算速度时间序列中超过90%分位数的数据点比例)——————————————\n",
    "    # 计算90%分位数\n",
    "    speed_series = speed_df[\"speed\"]\n",
    "    q90 = speed_series.quantile(0.9)\n",
    "    n = len(speed_series)\n",
    "    # 统计超过q90的数据点数量\n",
    "    count_above_q90 = len(speed_series[speed_series > q90])/n\n",
    "\n",
    "    res[\"NewSpeed_90%_quantile\"] = count_above_q90\n",
    "\n",
    "    #————————计算参数 Speed C3（计算速度时间序列的C3非线性度量)——————————————\n",
    "    #基于当前值与前两值自协方差的非线性度量(Schreiber&Schmitz，1997)\n",
    "    #用途:识别速度时间序列中的非线性动力学特征(如急加速/减速)。\n",
    "    speed_values = speed_series.dropna().values\n",
    "    n = len(speed_values)\n",
    "    mu = np.mean(speed_values)  # 均值\n",
    "    \n",
    "    # 计算三阶自协方差项的和\n",
    "    sum_c3 = 0.0\n",
    "    for i in range(2, n):\n",
    "        sum_c3 += speed_values[i] * speed_values[i-1] * speed_values[i-2]\n",
    "    \n",
    "    # 计算C3\n",
    "    c3 = (sum_c3 / (n - 2)) - mu**3\n",
    "    res[\"NewSpeed_C3\"] = c3\n",
    "\n",
    "    #————————计算参数 Speed change quantile variance（定义:速度变化量绝对值的方差。)——————————————\n",
    "    #用途:量化速度变化的波动性(如频繁加减速)\n",
    "\n",
    "    # 计算速度变化量（一阶差分）\n",
    "    speed_diff = speed_series.diff().dropna()\n",
    "    \n",
    "    # 取绝对值后计算方差\n",
    "    abs_diff = np.abs(speed_diff)\n",
    "    variance = np.var(abs_diff)\n",
    "    res[\"NewSpeed_change_quantile_variance\"] = variance\n",
    "\n",
    "    return res\n",
    "\n",
    "from shapely.geometry import Point, box\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "def compute_gaze_area(data:pd.DataFrame, radius=25):\n",
    "    # Create shapely circles for each gaze point\n",
    "    def create_circle(x, y, radius):\n",
    "        screen_box=box(0,0,1920,1080)\n",
    "        circle=Point(x, y).buffer(radius)\n",
    "        return circle.intersection(screen_box)\n",
    "\n",
    "    data['circle'] = data.apply(lambda row: create_circle(row['Gaze point X'], row['Gaze point Y'], radius), axis=1)\n",
    "\n",
    "    # Union of all circles\n",
    "    union_of_circles = unary_union(data['circle'].tolist())\n",
    "\n",
    "    # Area of the union of all circles\n",
    "    return union_of_circles.area\n",
    "\n",
    "def extract_eye_movement_type(eye_df):\n",
    "    eye_df=eye_df.copy()\n",
    "    res = {\n",
    "        \"GE_saccade_times\" : np.nan,\n",
    "        \"GE_fixation_times\" : np.nan,\n",
    "\n",
    "        \"GE_saccade_duration_mean\" : np.nan,\n",
    "        \"GE_saccade_duration_std\" : np.nan,\n",
    "        \"GE_saccade_duration_005quantiles\" : np.nan,\n",
    "        \"GE_saccade_duration_095quantiles\" : np.nan,\n",
    "        \"GE_saccade_duration_skewness\" : np.nan,\n",
    "        \"GE_saccade_duration_kurtosis\" : np.nan,\n",
    "\n",
    "        \"GE_fixation_duration_mean\" : np.nan,\n",
    "        \"GE_fixation_duration_std\" : np.nan,\n",
    "        \"GE_fixation_duration_005quantiles\" : np.nan,\n",
    "        \"GE_fixation_duration_095quantiles\" : np.nan,\n",
    "        \"GE_fixation_duration_skewness\" : np.nan,\n",
    "        \"GE_fixation_duration_kurtosis\" : np.nan,\n",
    "\n",
    "        \"GE_saccade_amplitude_mean\" : np.nan,\n",
    "        \"GE_saccade_amplitude_std\" : np.nan,\n",
    "        \"GE_saccade_amplitude_005quantiles\" : np.nan,\n",
    "        \"GE_saccade_amplitude_095quantiles\" : np.nan,\n",
    "        \"GE_saccade_amplitude_skewness\" : np.nan,\n",
    "        \"GE_saccade_amplitude_kurtosis\" : np.nan,\n",
    "\n",
    "        \"GE_saccade_peak_v_mean\" : np.nan,\n",
    "        \"GE_saccade_peak_v_std\" : np.nan,\n",
    "        \"GE_saccade_peak_v_005quantiles\" : np.nan,\n",
    "        \"GE_saccade_peak_v_095quantiles\" : np.nan,\n",
    "        \"GE_saccade_peak_v_skewness\" : np.nan,\n",
    "        \"GE_saccade_peak_v_kurtosis\" : np.nan,\n",
    "\n",
    "        \"GE_saccade_mean_v_mean\" : np.nan,\n",
    "        \"GE_saccade_mean_v_std\" : np.nan,\n",
    "        \"GE_saccade_mean_v_005quantiles\" : np.nan,\n",
    "        \"GE_saccade_mean_v_095quantiles\" : np.nan,\n",
    "        \"GE_saccade_mean_v_skewness\" : np.nan,\n",
    "        \"GE_saccade_mean_v_kurtosis\" : np.nan,\n",
    "    }\n",
    "    # 计算帧间的时间差\n",
    "    eye_df['dt'] = eye_df['timestamp'].diff()\n",
    "\n",
    "    # 计算帧间的位移\n",
    "    eye_df['dx'] = eye_df['Gaze point X'].diff()\n",
    "    eye_df['dy'] = eye_df['Gaze point Y'].diff()\n",
    "\n",
    "    # 计算瞬时速度 (像素/秒)\n",
    "    eye_df['velocity'] = np.sqrt(eye_df['dx']**2 + eye_df['dy']**2) / eye_df['dt']\n",
    "\n",
    "    # 给每个连续相同类型的段落编号\n",
    "    eye_df['segment_id'] = (eye_df['Eye movement type'] != eye_df['Eye movement type'].shift()).cumsum()\n",
    "    results = []\n",
    "\n",
    "    for seg_id, group in eye_df.groupby('segment_id'):\n",
    "        seg_type = group['Eye movement type'].iloc[0]\n",
    "        start_time = group['timestamp'].iloc[0]\n",
    "        end_time = group['timestamp'].iloc[-1]\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        if seg_type == 'Fixation':\n",
    "            results.append({\n",
    "                'type': 'Fixation',\n",
    "                'duration': duration,\n",
    "                'count': 1\n",
    "            })\n",
    "        elif seg_type == 'Saccade':\n",
    "            amplitude = np.sqrt((group['Gaze point X'].iloc[-1] - group['Gaze point X'].iloc[0])**2 +\n",
    "                                (group['Gaze point Y'].iloc[-1] - group['Gaze point Y'].iloc[0])**2)\n",
    "            peak_vel = group['velocity'].max()\n",
    "            mean_vel = group['velocity'].mean()\n",
    "            results.append({\n",
    "                'type': 'Saccade',\n",
    "                'duration': duration,\n",
    "                'count': 1,\n",
    "                'amplitude': amplitude,\n",
    "                'peak_velocity': peak_vel,\n",
    "                'mean_velocity': mean_vel\n",
    "            })\n",
    "\n",
    "    # 转成 DataFrame 方便汇总\n",
    "    res_df = pd.DataFrame(results)\n",
    "    # 分组数据\n",
    "    if res_df.empty:\n",
    "        return res\n",
    "    fix_df = res_df[res_df['type'] == 'Fixation']\n",
    "    sac_df = res_df[res_df['type'] == 'Saccade']\n",
    "\n",
    "    # 次数\n",
    "    res[\"GE_fixation_times\"] = len(fix_df)\n",
    "    res[\"GE_saccade_times\"] = len(sac_df)\n",
    "\n",
    "    # Fixation duration\n",
    "    if not fix_df.empty:\n",
    "        dur = fix_df['duration']\n",
    "        res[\"GE_fixation_duration_mean\"] = dur.mean()\n",
    "        res[\"GE_fixation_duration_std\"] = dur.std()\n",
    "        res[\"GE_fixation_duration_005quantiles\"] = dur.quantile(0.05)\n",
    "        res[\"GE_fixation_duration_095quantiles\"] = dur.quantile(0.95)\n",
    "        res[\"GE_fixation_duration_skewness\"] = skew(dur, nan_policy='omit')\n",
    "        res[\"GE_fixation_duration_kurtosis\"] = kurtosis(dur, nan_policy='omit')\n",
    "\n",
    "    # Saccade duration\n",
    "    if not sac_df.empty:\n",
    "        dur = sac_df['duration']\n",
    "        res[\"GE_saccade_duration_mean\"] = dur.mean()\n",
    "        res[\"GE_saccade_duration_std\"] = dur.std()\n",
    "        res[\"GE_saccade_duration_005quantiles\"] = dur.quantile(0.05)\n",
    "        res[\"GE_saccade_duration_095quantiles\"] = dur.quantile(0.95)\n",
    "        res[\"GE_saccade_duration_skewness\"] = skew(dur, nan_policy='omit')\n",
    "        res[\"GE_saccade_duration_kurtosis\"] = kurtosis(dur, nan_policy='omit')\n",
    "\n",
    "        # Saccade amplitude\n",
    "        amp = sac_df['amplitude']\n",
    "        res[\"GE_saccade_amplitude_mean\"] = amp.mean()\n",
    "        res[\"GE_saccade_amplitude_std\"] = amp.std()\n",
    "        res[\"GE_saccade_amplitude_005quantiles\"] = amp.quantile(0.05)\n",
    "        res[\"GE_saccade_amplitude_095quantiles\"] = amp.quantile(0.95)\n",
    "        res[\"GE_saccade_amplitude_skewness\"] = skew(amp, nan_policy='omit')\n",
    "        res[\"GE_saccade_amplitude_kurtosis\"] = kurtosis(amp, nan_policy='omit')\n",
    "\n",
    "        # Saccade peak velocity\n",
    "        pv = sac_df['peak_velocity']\n",
    "        res[\"GE_saccade_peak_v_mean\"] = pv.mean()\n",
    "        res[\"GE_saccade_peak_v_std\"] = pv.std()\n",
    "        res[\"GE_saccade_peak_v_005quantiles\"] = pv.quantile(0.05)\n",
    "        res[\"GE_saccade_peak_v_095quantiles\"] = pv.quantile(0.95)\n",
    "        res[\"GE_saccade_peak_v_skewness\"] = skew(pv, nan_policy='omit')\n",
    "        res[\"GE_saccade_peak_v_kurtosis\"] = kurtosis(pv, nan_policy='omit')\n",
    "\n",
    "        # Saccade mean velocity\n",
    "        mv = sac_df['mean_velocity']\n",
    "        res[\"GE_saccade_mean_v_mean\"] = mv.mean()\n",
    "        res[\"GE_saccade_mean_v_std\"] = mv.std()\n",
    "        res[\"GE_saccade_mean_v_005quantiles\"] = mv.quantile(0.05)\n",
    "        res[\"GE_saccade_mean_v_095quantiles\"] = mv.quantile(0.95)\n",
    "        res[\"GE_saccade_mean_v_skewness\"] = skew(mv, nan_policy='omit')\n",
    "        res[\"GE_saccade_mean_v_kurtosis\"] = kurtosis(mv, nan_policy='omit')\n",
    "\n",
    "    return res\n",
    "\n",
    "def extract_eye(eye_df):\n",
    "    res = {\n",
    "        \"LASTE_eye_speed_x_mean\" : np.nan,\n",
    "        \"LASTE_eye_speed_y_mean\" : np.nan,\n",
    "        \"LASTE_eye_speed_eye_mean\" : np.nan,\n",
    "        \"LASTE_eye_speed_x_max\" : np.nan,\n",
    "        \"LASTE_eye_speed_y_max\" : np.nan,\n",
    "        \"LASTE_eye_speed_eye_max\" : np.nan,\n",
    "        #\"LASTE_gaze_area_ratio\" : np.nan,\n",
    "    }\n",
    "    if eye_df.shape[0]==0:\n",
    "        return res\n",
    "    \n",
    "    data = np.abs(eye_df[\"Gaze point X_derivative\"])\n",
    "\n",
    "    if len(data) > 0:\n",
    "        res[\"LASTE_eye_speed_x_mean\"] = np.nanmean(data)\n",
    "    else:\n",
    "        res[\"LASTE_eye_speed_x_mean\"] = np.nan\n",
    "\n",
    "    data = np.abs(eye_df[\"Gaze point Y_derivative\"])\n",
    "\n",
    "    if len(data) > 0:\n",
    "        res[\"LASTE_eye_speed_y_mean\"] = np.nanmean(data)\n",
    "    else:\n",
    "        res[\"LASTE_eye_speed_y_mean\"] = np.nan\n",
    "\n",
    "    # res[\"LASTE_eye_speed_x_mean\"] = np.nanmean(np.abs(eye_df[\"Gaze point X_derivative\"]))\n",
    "    # res[\"LASTE_eye_speed_y_mean\"] = np.nanmean(np.abs(eye_df[\"Gaze point Y_derivative\"]))\n",
    "    res[\"LASTE_eye_speed_eye_mean\"] = np.nanmean(np.abs(eye_df[\"eyemovement_speed\"]))\n",
    "    res[\"LASTE_eye_speed_x_max\"] = np.max(np.abs(eye_df[\"Gaze point X_derivative\"]))\n",
    "    res[\"LASTE_eye_speed_y_max\"] = np.max(np.abs(eye_df[\"Gaze point Y_derivative\"]))\n",
    "    res[\"LASTE_eye_speed_eye_max\"] = np.max(np.abs(eye_df[\"eyemovement_speed\"]))\n",
    "    #res[\"gaze_area_ratio\"] = np.float32(compute_gaze_area(eye_df) / (1920*1080))\n",
    "    return res\n",
    "\n",
    "#普通统计特征计算\n",
    "def get_stats(data, key_suffix: str = None, feature_type: str = None):\n",
    "    \"\"\"\n",
    "    Function defining the statistical measures considered for aggregation\n",
    "    :return: (pd.DataFrame) data of aggregated featues with column 'num_samples'\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'mean': np.nan,\n",
    "        'std': np.nan,\n",
    "        'q5': np.nan,\n",
    "        'q95': np.nan,\n",
    "        #'power': np.nan,\n",
    "        'skewness': np.nan,\n",
    "        'kurtosis': np.nan,\n",
    "    }\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        results['mean'] = np.mean(data)\n",
    "        results['std'] = np.std(data)\n",
    "        results['q5'] = np.quantile(data, 0.05)\n",
    "        results['q95'] = np.quantile(data, 0.95)\n",
    "        if np.std(data) < 1e-8:\n",
    "            results['skewness'] = np.nan\n",
    "        else:\n",
    "            results['skewness'] = skew(data)\n",
    "        if np.std(data) < 1e-8:\n",
    "            results['kurtosis'] = np.nan\n",
    "        else:\n",
    "            results['kurtosis'] = kurtosis(data)\n",
    "        # results['skewness'] = skew(data)\n",
    "        # results['kurtosis'] = kurtosis(data)\n",
    "    \n",
    "    # 拼接前缀和后缀\n",
    "    prefix_map = {\n",
    "        'pedal': 'STATSP_',\n",
    "        'speed': 'STATSS_',\n",
    "        'eyemovement': 'STATSE_',\n",
    "        'head': 'STATSH_'\n",
    "    }\n",
    "\n",
    "    full_prefix = prefix_map.get(feature_type, '')  # 默认无前缀\n",
    "    if key_suffix is not None:\n",
    "        results = {f\"{full_prefix}{key_suffix}_{k}\": v for k, v in results.items()}\n",
    "    else:\n",
    "        results = {f\"{full_prefix}{k}\": v for k, v in results.items()}\n",
    "    \n",
    "    return results\n",
    "\n",
    "#普通统计特征计算\n",
    "def get_stats_new(data, key_suffix: str = None, feature_type: str = None):\n",
    "    \"\"\"\n",
    "    Function defining the statistical measures considered for aggregation\n",
    "    :return: (pd.DataFrame) data of aggregated featues with column 'num_samples'\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'ptp': np.nan,#峰峰值\n",
    "        'median': np.nan, #中位数\n",
    "        'energy': np.nan, #能量\n",
    "        'rms': np.nan, #均方根\n",
    "        'lineintegral': np.nan, #线积分 相邻数据点差值的绝对值之和\n",
    "        'n_sign_changes': np.nan, #符号变化次数\n",
    "        'iqr': np.nan, #四分位距\n",
    "        'iqr_5_95': np.nan #5%-95%分位距\n",
    "    }\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        results['ptp'] = np.ptp(data)\n",
    "        results['median'] = np.median(data)\n",
    "        results['energy'] = np.sum(data ** 2)\n",
    "        results['rms'] = np.sqrt(np.sum(data ** 2) / len(data))\n",
    "        results['lineintegral'] = np.abs(np.diff(data)).sum()\n",
    "        results['n_sign_changes'] = np.sum(np.diff(np.sign(data)) != 0)\n",
    "        results['iqr'] = np.subtract(*np.nanpercentile(data, [75, 25]))\n",
    "        results['iqr_5_95'] = np.subtract(*np.nanpercentile(data, [95, 5]))\n",
    "    \n",
    "    # 拼接前缀和后缀\n",
    "    prefix_map = {\n",
    "        'pedal': 'STATSPNEW_',\n",
    "        'speed': 'STATSSNEW_',\n",
    "        'eyemovement': 'STATSENEW_',\n",
    "        'head': 'STATSHNEW_'\n",
    "    }\n",
    "\n",
    "    full_prefix = prefix_map.get(feature_type, '')  # 默认无前缀\n",
    "    if key_suffix is not None:\n",
    "        results = {f\"{full_prefix}{key_suffix}_{k}\": v for k, v in results.items()}\n",
    "    else:\n",
    "        results = {f\"{full_prefix}{k}\": v for k, v in results.items()}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f7b2b",
   "metadata": {},
   "source": [
    "## 滑动窗口和特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#滑动窗口\n",
    "def get_sliding_window(data: pd.Series,speed_data, eyemovement_data, start_end_list):\n",
    "    \"\"\"\n",
    "    Function to get aggregated features in parallel implementation\n",
    "    data: (pd.DataFrame) data of whole dataframe to aggregate (no columns 'id', 'label', 'scenario')\n",
    "    epoch_width: (int) time window in [s] to aggregate\n",
    "    i: (int) index of start frame of aggregation\n",
    "    :return: (pd.DataFrame) data of aggregated features with column 'num_samples'\n",
    "    \"\"\"\n",
    "    min_timestamp = start_end_list[0]\n",
    "    max_timestamp = start_end_list[1]\n",
    "\n",
    "    results = {\n",
    "        'datetime': min_timestamp,\n",
    "    }    \n",
    "    \n",
    "    relevant_data = data[(data['timestamp'] > min_timestamp) \n",
    "                         & (data['timestamp'] < max_timestamp)]\n",
    "    \n",
    "    relevant_speed_data = speed_data[(speed_data['timestamp'] > min_timestamp) \n",
    "                         & (speed_data['timestamp'] < max_timestamp)]\n",
    "    \n",
    "    relevant_eyemovement_data = eyemovement_data[(eyemovement_data['timestamp'] > min_timestamp) \n",
    "                         & (eyemovement_data['timestamp'] < max_timestamp)]\n",
    "    \n",
    "    #-----------pedal-----------\n",
    "    for column in relevant_data.columns:\n",
    "        if column == \"timestamp\" or column == \"person_id\":\n",
    "            continue\n",
    "        column_results = get_stats(relevant_data[column], column,'pedal')\n",
    "        results.update(column_results)\n",
    "        column_results_new = get_stats_new(relevant_data[column], column,'pedal')\n",
    "        results.update(column_results_new)\n",
    "        \n",
    "    #傅里叶变换、小波变换\n",
    "    fft_results = get_fft_stats(relevant_data)\n",
    "    results.update(fft_results)\n",
    "\n",
    "    wavelet_results = extract_wavelet_features(relevant_data)\n",
    "    results.update(wavelet_results)\n",
    "\n",
    "    pedal_para = extract_pedal(relevant_data)\n",
    "    results.update(pedal_para)\n",
    "\n",
    "    new_pedal_para = extract_new_pedal_feature(relevant_data)\n",
    "    results.update(new_pedal_para)\n",
    "    \n",
    "    #------------speed-------------\n",
    "    new_speed_para = extract_new_speed_feature(relevant_speed_data)\n",
    "    results.update(new_speed_para)\n",
    "\n",
    "    for column in relevant_speed_data.columns:\n",
    "        if column == \"timestamp\" or column == \"person_id\":\n",
    "            continue\n",
    "        column_results = get_stats(relevant_speed_data[column], column,'speed')\n",
    "        results.update(column_results)\n",
    "        column_results_new = get_stats_new(relevant_speed_data[column], column,'speed')\n",
    "        results.update(column_results_new)\n",
    "\n",
    "    #------------eye--------------    \n",
    "    for column in relevant_eyemovement_data.columns:\n",
    "        if column == \"timestamp\" or column == \"person_id\" or column == \"Eye movement type\" or column ==\"Eye position left X (DACSmm)\" or column == \"Eye position left Y (DACSmm)\" or column == \"Eye position left Z (DACSmm)\" or column == \"Eye position right X (DACSmm)\" or column == \"Eye position right Y (DACSmm)\" or column == \"Eye position right Z (DACSmm)\": \n",
    "            continue\n",
    "        if column.startswith(\"Head\"):\n",
    "            column_results = get_stats(relevant_eyemovement_data[column], column,'head')\n",
    "            results.update(column_results)\n",
    "            column_results_new = get_stats_new(relevant_eyemovement_data[column], column,'head')\n",
    "            results.update(column_results_new)\n",
    "        else:\n",
    "            column_results = get_stats(relevant_eyemovement_data[column], column,'eyemovement')\n",
    "            results.update(column_results)\n",
    "            column_results_new = get_stats_new(relevant_eyemovement_data[column], column,'eyemovement')\n",
    "            results.update(column_results_new)\n",
    "\n",
    "    eye_movement_type = extract_eye_movement_type(relevant_eyemovement_data)\n",
    "    results.update(eye_movement_type)\n",
    "    # eye_para = extract_eye(relevant_eyemovement_data)\n",
    "    # results.update(eye_para)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_features(data: pd.DataFrame, speed_data, eyemovement_data, num_cores: int = 0,start_row= 0):\n",
    "    \"\"\"\n",
    "    Function to get aggregated features in parallel implementation\n",
    "    data: (pd.DataFrame) data of whole dataframe to aggregate (no columns 'id', 'label', 'scenario')\n",
    "    epoch_width: (int) time window in [s] to aggregate\n",
    "    num_cores: (int) number of CPU cores to be used\n",
    "    :return: (pd.DataFrame) data of aggregated features\n",
    "    \"\"\"\n",
    "\n",
    "    input_data = data.copy()\n",
    "    input_speed_data = speed_data.copy()\n",
    "    input_eyemovement_data = eyemovement_data.copy()    \n",
    "\n",
    "    windows = []\n",
    "    total_rows = len(input_data)\n",
    "\n",
    "    for start in range(0, total_rows - WINDOW_SIZE, WINDOW_STEP):\n",
    "        start_timestamp = float(input_data.iloc[start]['timestamp'])\n",
    "        end_timestamp = float(input_data.iloc[start + WINDOW_SIZE]['timestamp'])\n",
    "        windows.append([start_timestamp,end_timestamp])\n",
    "\n",
    "    results = []\n",
    "    if len(windows)==0:\n",
    "        return results\n",
    "    for k in windows:\n",
    "        results.append(get_sliding_window(input_data,input_speed_data, input_eyemovement_data, k))\n",
    "    \n",
    "    results = pd.DataFrame(list(filter(None, results)))  # filter out None values\n",
    "    results.set_index('datetime', inplace=True)\n",
    "    results.sort_index(inplace=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df3d95",
   "metadata": {},
   "source": [
    "## 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9406a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from config.config_loader import load_config\n",
    "# import os\n",
    "\n",
    "# road_type = \"straight\"\n",
    "# CONFIG_DATA = load_config(os.path.join(\"config\", \"feature_engineering_config.json\"))\n",
    "# NC_number = CONFIG_DATA[\"NC_number\"]\n",
    "# PD_number = CONFIG_DATA[\"PD_number\"]\n",
    "\n",
    "# result = pd.DataFrame()\n",
    "# # WINDOW_SIZE_LIST = [1275,1275*2,1275*3]  #120\n",
    "# # size_list =['60','120','180']\n",
    "# # WINDOW_STEP_LIST = [11,21,43,108,212,319,637]\n",
    "# # step_list = ['0.5','1','2','5','10','15','30']\n",
    "# # 0.5s 11\n",
    "# # 1s 21\n",
    "# # 2s 43\n",
    "# # 5s 108\n",
    "# WINDOW_SIZE_LIST = [1275*2]  #120\n",
    "# size_list =['120']\n",
    "# WINDOW_STEP_LIST = [21]\n",
    "# step_list = ['1']\n",
    "\n",
    "# for i in range(len(WINDOW_SIZE_LIST)):\n",
    "#     for j in range(len(WINDOW_STEP_LIST)):\n",
    "#         output_name= f\"feature_{size_list[i]}_{step_list[j]}.csv\"\n",
    "#         print(\"processing\"+output_name+\"\\n\")\n",
    "#         WINDOW_SIZE = WINDOW_SIZE_LIST[i]\n",
    "#         WINDOW_STEP = WINDOW_STEP_LIST[j]\n",
    "#         if i == 1 and (j == 1 or j==2 or j ==3 or j ==4):\n",
    "#             continue\n",
    "#         for type in ['PD','NC']:\n",
    "#             if type == 'PD':\n",
    "#                 person_amount = PD_number\n",
    "#             else:\n",
    "#                 person_amount = NC_number\n",
    "\n",
    "#             for person_num in range(person_amount):\n",
    "#                 print(\"在处理第\"+str(person_num+1)+\"个文件\")\n",
    "\n",
    "#                 data_type = \"pedal\"        \n",
    "#                 pedal_data = pd.read_csv(\n",
    "#                                     f\"sliced_data/{type}/{type}{person_num + 1}/{road_type}_{data_type}_ex2.csv\"\n",
    "#                                 )\n",
    "#                 #print(f\"new_seg_data/{type}/{type}{person_num + 1}/{road_type}/{road_type}_{data_type}.csv\")\n",
    "#                 #print(pedal_data)\n",
    "#                 data_type = \"speed\"        \n",
    "#                 speed_data = pd.read_csv(\n",
    "#                                     f\"sliced_data/{type}/{type}{person_num + 1}/{road_type}_{data_type}_ex2.csv\"\n",
    "#                                 )\n",
    "                \n",
    "#                 data_type = \"eyemovement\"        \n",
    "#                 eyemovement_data = pd.read_csv(\n",
    "#                                     f\"sliced_data/{type}/{type}{person_num + 1}/{road_type}_{data_type}_ex2.csv\"\n",
    "#                                 )\n",
    "\n",
    "#                 res = get_features(pedal_data,speed_data,eyemovement_data,0,0)\n",
    "\n",
    "#                 res = pd.concat([\n",
    "#                     res,\n",
    "#                     pd.DataFrame({\n",
    "#                         'participant': [f\"{type} P{person_num + 1}\"] * len(res),\n",
    "#                         'experiment': ['ex2'] * len(res),\n",
    "#                         'label':1 if type =='PD' else 0\n",
    "#                     }, index=res.index)\n",
    "#                 ], axis=1)\n",
    "\n",
    "#                 result = pd.concat([result, res], axis=0)\n",
    "\n",
    "#         result.to_csv(output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6694af7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processingturn_feature_120_1.csv\n",
      "\n",
      "在处理第1个文件\n",
      "在处理第2个文件\n",
      "在处理第3个文件\n",
      "在处理第4个文件\n",
      "在处理第5个文件\n",
      "在处理第6个文件\n",
      "在处理第7个文件\n",
      "在处理第8个文件\n",
      "在处理第9个文件\n",
      "在处理第10个文件\n",
      "在处理第11个文件\n",
      "在处理第12个文件\n",
      "在处理第13个文件\n",
      "在处理第14个文件\n",
      "在处理第15个文件\n",
      "在处理第16个文件\n",
      "在处理第17个文件\n",
      "在处理第18个文件\n",
      "在处理第19个文件\n",
      "在处理第20个文件\n",
      "在处理第21个文件\n",
      "在处理第22个文件\n",
      "在处理第23个文件\n",
      "在处理第24个文件\n",
      "在处理第25个文件\n",
      "在处理第26个文件\n",
      "在处理第27个文件\n",
      "在处理第1个文件\n",
      "在处理第2个文件\n",
      "在处理第3个文件\n",
      "在处理第4个文件\n",
      "在处理第5个文件\n",
      "在处理第6个文件\n",
      "在处理第7个文件\n",
      "在处理第8个文件\n",
      "在处理第9个文件\n",
      "在处理第10个文件\n",
      "在处理第11个文件\n",
      "在处理第12个文件\n",
      "在处理第13个文件\n",
      "在处理第14个文件\n",
      "在处理第15个文件\n",
      "在处理第16个文件\n",
      "在处理第17个文件\n",
      "在处理第18个文件\n",
      "在处理第19个文件\n",
      "在处理第20个文件\n",
      "在处理第21个文件\n",
      "在处理第22个文件\n",
      "在处理第23个文件\n",
      "在处理第24个文件\n",
      "在处理第25个文件\n",
      "在处理第26个文件\n",
      "在处理第27个文件\n",
      "在处理第28个文件\n",
      "在处理第29个文件\n",
      "在处理第30个文件\n",
      "在处理第31个文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config.config_loader import load_config\n",
    "import os\n",
    "\n",
    "road_type = \"turn\"\n",
    "CONFIG_DATA = load_config(os.path.join(\"config\", \"feature_engineering_config.json\"))\n",
    "NC_number = CONFIG_DATA[\"NC_number\"]\n",
    "PD_number = CONFIG_DATA[\"PD_number\"]\n",
    "\n",
    "result = pd.DataFrame()\n",
    "# WINDOW_SIZE_LIST = [1275,1275*2,1275*3]  #120\n",
    "# size_list =['60','120','180']\n",
    "# WINDOW_STEP_LIST = [11,21,43,108,212,319,637]\n",
    "# step_list = ['0.5','1','2','5','10','15','30']\n",
    "# 0.5s 11\n",
    "# 1s 21\n",
    "# 2s 43\n",
    "# 5s 108\n",
    "WINDOW_SIZE_LIST = [1275*2]  #120\n",
    "size_list =['120']\n",
    "WINDOW_STEP_LIST = [21]\n",
    "step_list = ['1']\n",
    "\n",
    "for i in range(len(WINDOW_SIZE_LIST)):\n",
    "    for j in range(len(WINDOW_STEP_LIST)):\n",
    "        output_name= f\"turn_feature_{size_list[i]}_{step_list[j]}.csv\"\n",
    "        print(\"processing\"+output_name+\"\\n\")\n",
    "        WINDOW_SIZE = WINDOW_SIZE_LIST[i]\n",
    "        WINDOW_STEP = WINDOW_STEP_LIST[j]\n",
    "        if i == 1 and (j == 1 or j==2 or j ==3 or j ==4):\n",
    "            continue\n",
    "        for type in ['PD','NC']:\n",
    "            if type == 'PD':\n",
    "                person_amount = PD_number\n",
    "            else:\n",
    "                person_amount = NC_number\n",
    "\n",
    "            for person_num in range(person_amount):\n",
    "                print(\"在处理第\"+str(person_num+1)+\"个文件\")\n",
    "\n",
    "                data_type = \"pedal\"        \n",
    "                pedal_data = pd.read_csv(\n",
    "                                    f\"sliced_data/{type}/{type}{person_num + 1}/{road_type}_{data_type}_ex2.csv\"\n",
    "                                )\n",
    "                #print(f\"new_seg_data/{type}/{type}{person_num + 1}/{road_type}/{road_type}_{data_type}.csv\")\n",
    "                #print(pedal_data)\n",
    "                data_type = \"speed\"        \n",
    "                speed_data = pd.read_csv(\n",
    "                                    f\"sliced_data/{type}/{type}{person_num + 1}/{road_type}_{data_type}_ex2.csv\"\n",
    "                                )\n",
    "                \n",
    "                data_type = \"eyemovement\"        \n",
    "                eyemovement_data = pd.read_csv(\n",
    "                                    f\"sliced_data/{type}/{type}{person_num + 1}/{road_type}_{data_type}_ex2.csv\"\n",
    "                                )\n",
    "\n",
    "                res = get_features(pedal_data,speed_data,eyemovement_data,0,0)\n",
    "                if  len(res)==0:\n",
    "                    continue\n",
    "                res = pd.concat([\n",
    "                    res,\n",
    "                    pd.DataFrame({\n",
    "                        'participant': [f\"{type} P{person_num + 1}\"] * len(res),\n",
    "                        'experiment': ['ex2'] * len(res),\n",
    "                        'label':1 if type =='PD' else 0\n",
    "                    }, index=res.index)\n",
    "                ], axis=1)\n",
    "\n",
    "                result = pd.concat([result, res], axis=0)\n",
    "\n",
    "        result.to_csv(output_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca2992",
   "metadata": {},
   "source": [
    "## 整段数据不切分 实验123 要修改地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c9fdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from config.config_loader import load_config\n",
    "# import os\n",
    "\n",
    "# CONFIG_DATA = load_config(os.path.join(\"config\", \"feature_engineering_config.json\"))\n",
    "# NC_number = CONFIG_DATA[\"NC_number\"]\n",
    "# PD_number = CONFIG_DATA[\"PD_number\"]\n",
    "# result = pd.DataFrame()\n",
    "\n",
    "# WINDOW_SIZE_LIST = [1275*2]  #120\n",
    "# size_list =['120']\n",
    "# WINDOW_STEP_LIST = [21]\n",
    "# step_list = ['1']\n",
    "# output_name= f\"feature_{size_list[0]}_{step_list[0]}.csv\"\n",
    "\n",
    "# for type in ['PD','NC']:\n",
    "#     for ex in ['ex1','ex2']:\n",
    "\n",
    "#         if type == 'PD':\n",
    "#             person_amount = PD_number\n",
    "#         else:\n",
    "#             person_amount = NC_number\n",
    "\n",
    "#         for person_num in range(person_amount):\n",
    "#             print(\"在处理第\"+str(person_num+1)+\"个文件\")\n",
    "\n",
    "#             data_type = \"pedal\"        \n",
    "#             pedal_data = pd.read_csv(\n",
    "#                                 f\"sliced_data/{type}/{type}{person_num + 1}/whole_{data_type}_{ex}.csv\"\n",
    "#                             )\n",
    "\n",
    "#             data_type = \"speed\"        \n",
    "#             speed_data = pd.read_csv(\n",
    "#                                 f\"sliced_data/{type}/{type}{person_num + 1}/whole_{data_type}_{ex}.csv\"\n",
    "#                             )\n",
    "            \n",
    "#             data_type = \"eyemovement\"        \n",
    "#             eyemovement_data = pd.read_csv(\n",
    "#                                 f\"sliced_data/{type}/{type}{person_num + 1}/whole_{data_type}_{ex}.csv\"\n",
    "#                             )\n",
    "\n",
    "#             res = get_features(pedal_data,speed_data,eyemovement_data,0,0)\n",
    "\n",
    "#             res = pd.concat([\n",
    "#                 res,\n",
    "#                 pd.DataFrame({\n",
    "#                     'participant': [f\"{type} P{person_num + 1}\"] * len(res),\n",
    "#                     'experiment': [ex] * len(res),\n",
    "#                     'label':1 if type =='PD' else 0\n",
    "#                 }, index=res.index)\n",
    "#             ], axis=1)\n",
    "\n",
    "#             result = pd.concat([result, res], axis=0)\n",
    "\n",
    "# result.to_csv(output_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
